<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Ruihai Wu</title>
  
  <meta name="author" content="Ruihai Wu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⭐️</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Ruihai Wu</name>
              </p>
              <p>I am a final-year PhD Candidate in <a href="https://cfcs.pku.edu.cn/english/"> Center on Frontiers of Computing Studies (CFCS)</a> at <a href="https://english.pku.edu.cn/">Peking University</a>.
                , advised by Professor <a href="https://zsdonghao.github.io/">Hao Dong</a>.
              </p>
              <p>
                  My research interests include computer vision and robotics.
              </p>
              <p>
                Email: wuruihai [at] pku.edu.cn
              </p>
              <p style="text-align:center">
                <a href="mailto:wuruihai@pku.edu.cn">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=qVyvE6UAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/warshallrho/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/selfie.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/selfie.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>&nbsp&nbsp&nbsp&nbsp&nbsp (* denotes equal contribution)
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <tr onmouseout="garmentpile_stop()" onmouseover="garmentpile_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='garmentpile_image'>
                  <img src='images/garmentpile.jpg' width="187"></div>
                <img src='images/garmentpile.jpg' width="187">
              </div>
              <script type="text/javascript">
                function garmentpile_start() {
                  document.getElementById('garmentpile_image').style.opacity = "1";
                }

                function garmentpile_stop() {
                  document.getElementById('garmentpile_image').style.opacity = "0";
                }
                garmentpile_stop()
              </script>
            </td>

            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://garmentpile.github.io/">
              <papertitle>GarmentPile: Point-Level Visual Affordance Guided Retrieval and Adaptation for Cluttered Garments Manipulation
              </papertitle>
              </a>
              <br>
              <strong>Ruihai Wu</strong>*,
              <a href="https://alwaysleepy.github.io/">Ziyu Zhu</a>*,
              Yuran Wang*,
              <a href="https://yuechen0614.github.io/homepage/">Yue Chen</a>,
              Jiarui Wang,
              <a href="http://zsdonghao.github.io/">Hao Dong</a>
              <br>
              <em>CVPR 2025</em>
              <br>
              <a href="https://garmentpile.github.io/">project page</a>
              /
              <a href="https://arxiv.org/abs/2503.09243">paper</a>
<!--              paper (coming soon)-->
              /
              <a href="https://github.com/AlwaySleepy/Garment-Pile">code</a>
<!--              code (coming soon)-->
              /
<!--              video (coming soon)-->
              <a href="https://garmentpile.github.io/static/videos/Introduction.mp4">video</a>
              <p></p>
              <p>We propose to learn point-level affordance to model the complex space and multi-modal manipulation candidates of garment piles, with novel designs for the awareness of garment geometry, structure, inter-object relations, and further adaptation. </p>
            </td>
          </tr>


          <tr onmouseout="biassemble_stop()" onmouseover="biassemble_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='biassemble_image'>
                  <img src='images/biassemble.png' width="187"></div>
                <img src='images/biassemble.png' width="187">
              </div>
              <script type="text/javascript">
                function biassemble_start() {
                  document.getElementById('biassemble_image').style.opacity = "1";
                }

                function biassemble_stop() {
                  document.getElementById('biassemble_image').style.opacity = "0";
                }
                biassemble_stop()
              </script>
            </td>

            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://biassemble.github.io/">
              <papertitle>BiAssemble: Learning Collaborative Affordance for Bimanual Geometric Assembly
              </papertitle>
              </a>
              <br>
              <a href="https://sxy7147.github.io/">Yan Shen</a>*,
              <strong>Ruihai Wu</strong>*,
              Yubin Ke,
              Xinyuan Song,
              Zeyi Li,
              <a href="https://scholar.google.com/citations?hl=en&user=vkQ5_LIAAAAJ">Xiaoqi Li</a>,
              <a href="https://hwfan.io/about-me/">Hongwei Fan</a>,
              <a href="https://luhr2003.github.io/">Haoran Lu</a>,
              <a href="http://zsdonghao.github.io/">Hao Dong</a>
              <br>
              <em>ICML 2025</em>
              <br>
              <a href="https://biassemble.github.io/">project page</a>
              /
<!--              <a href="https://arxiv.org/abs/2503.09243">paper</a>-->
              paper (coming soon)
              /
<!--              <a href="https://github.com/AlwaySleepy/Garment-Pile">code</a>-->
              code (coming soon)
              /
              video (coming soon)
<!--              <a href="https://garmentpile.github.io/static/videos/Introduction.mp4">video</a>-->
              <p></p>
              <p>We exploit the geometric generalization of point-level affordance, learning affordance aware of bimanual collaboration in geometric assembly with long-horizon action sequences. </p>
            </td>
          </tr>


          <tr onmouseout="adamanip_stop()" onmouseover="adamanip_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='adamanip_image'>
                  <img src='images/adamanip.png' width="187"></div>
                <img src='images/adamanip.png' width="187">
              </div>
              <script type="text/javascript">
                function adamanip_start() {
                  document.getElementById('adamanip_image').style.opacity = "1";
                }

                function adamanip_stop() {
                  document.getElementById('adamanip_image').style.opacity = "0";
                }
                adamanip_stop()
              </script>
            </td>

            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://adamanip.github.io/">
              <papertitle>AdaManip: Adaptive Articulated Object Manipulation Environments and Policy Learning
              </papertitle>
              </a>
              <br>
              <a href="https://yuanfei-wang.github.io/">Yuanfei Wang</a>*,
              Xiaojie Zhang*,
              <strong>Ruihai Wu</strong>*,
              Yu Li,
              <a href="https://sxy7147.github.io/">Yan Shen</a>,
              <a href="https://aaronanima.github.io/">Mingdong Wu</a>,
              Zhaofeng He,
              <a href="https://cfcs.pku.edu.cn/english/people/faculty/yizhouwang/index.htm">Yizhou Wang</a>,
              <a href="http://zsdonghao.github.io/">Hao Dong</a>
              <br>
              <em>ICLR 2025</em>
              <br>
              <a href="https://adamanip.github.io/">project page</a>
              /
              <a href="https://arxiv.org/abs/2502.11124">paper</a>
<!--              paper (coming soon)-->
              /
              <a href="https://github.com/yuanfei-Wang/AdaManip/">code</a>
<!--              code (coming soon)-->
              /
<!--              video (coming soon)-->
              <a href="https://www.youtube.com/watch?v=YPJsvSO83RE">video</a>
              <p></p>
              <p>We propose adaptive assets, environment and policy to manipulate articulated objects with diverse mechanisms. </p>
            </td>
          </tr>

          <tr onmouseout="etseed_stop()" onmouseover="etseed_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='etseed_image'>
                  <img src='images/etseed.png' width="187"></div>
                <img src='images/etseed.png' width="187">
              </div>
              <script type="text/javascript">
                function etseed_start() {
                  document.getElementById('etseed_image').style.opacity = "1";
                }

                function etseed_stop() {
                  document.getElementById('etseed_image').style.opacity = "0";
                }
                etseed_stop()
              </script>
            </td>

            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://et-seed.github.io/">
              <papertitle>ET-SEED: Efficient Trajectory-Level SE(3) Equivariant Diffusion Policy
              </papertitle>
              </a>
              <br>
              <a href="https://crtie.github.io/">Chenrui Tie</a>*,
              <a href="https://yuechen0614.github.io/homepage/">Yue Chen</a>*,
              <strong>Ruihai Wu</strong>*,
              Boxuan Dong,
              Zeyi Li,
              <a href="https://chongkaigao.com/">Chongkai Gao</a>,
              <a href="http://zsdonghao.github.io/">Hao Dong</a>
              <br>
              <em>ICLR 2025</em>
              <br>
              <a href="https://et-seed.github.io/">project page</a>
              /
              <a href="https://arxiv.org/abs/2411.03990">paper</a>
<!--              paper (coming soon)-->
              /
              <a href="https://github.com/Cold114514/ET-SEED">code</a>
<!--              code (coming soon)-->
              /
<!--              video (coming soon)-->
              <a href="https://et-seed.github.io/static/videos/ET-SEED_sup_video.mp4">video</a>
              <p></p>
              <p>We theoretically extend equivariant Markov kernels and simplify the condition of equivariant diffusion process, thereby significantly improving training efficiency for trajectory-level SE(3) equivariant diffusion policy in an end-to-end manner. </p>
            </td>
          </tr>


          <tr onmouseout="garment_stop()" onmouseover="garment_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='garment_image'>
                  <img src='images/garment.jpg' width="187"></div>
                <img src='images/garment.jpg' width="187">
              </div>
              <script type="text/javascript">
                function garment_start() {
                  document.getElementById('garment_image').style.opacity = "1";
                }

                function garment_stop() {
                  document.getElementById('garment_image').style.opacity = "0";
                }
                garment_stop()
              </script>
            </td>

            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://warshallrho.github.io/unigarmentmanip/">
              <papertitle>UniGarmentManip: A Unified Framework for Category-Level Garment Manipulation via Dense Visual Correspondence
              </papertitle>
              </a>
              <br>
              <strong>Ruihai Wu</strong>*,
              <a href="https://luhr2003.github.io/">Haoran Lu</a>*,
              Yiyan Wang,
              Yubo Wang,
              <a href="http://zsdonghao.github.io/">Hao Dong</a>
              <br>
              <em>CVPR 2024</em>
              <br>
               <b style="color: red;">Nomination (top3) of Outstanding Youth Paper Award</b> at <a href="https://ceai.caai.cn/">China Embodied AI Conference (CEAI) 2025</a>
              <br>
               <b style="color: red;">Spotlight Presentation</b> at <a href="https://deformable-workshop.github.io/icra2024/">ICRA 2024 Workshop on Deformable Object Manipulation</a>
              <br>
              <a href="https://warshallrho.github.io/unigarmentmanip/">project page</a>
              /
              <a href="https://arxiv.org/abs/2405.06903">paper</a>
<!--              paper (coming soon)-->
              /
              <a href="https://github.com/luhr2003/UniGarmentManip/">code</a>
<!--              code (coming soon)-->
              /
<!--              video (coming soon)-->
              <a href="https://www.youtube.com/watch?v=N5NYt-XJDOs">video</a>
              <p></p>
              <p>We propose to learn dense visual correspondence for diverse garment manipulation tasks with category-level generalization using only one- or few-shot human demonstrations. </p>
            </td>
          </tr>

          <tr onmouseout="clutter_stop()" onmouseover="clutter_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='clutter_image'>
                  <img src='images/clutter.png' width="187"></div>
                <img src='images/clutter.png' width="187">
              </div>
              <script type="text/javascript">
                function clutter_start() {
                  document.getElementById('clutter_image').style.opacity = "1";
                }

                function clutter_stop() {
                  document.getElementById('clutter_image').style.opacity = "0";
                }
                clutter_stop()
              </script>
            </td>

            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://lyttttt3333.github.io/broadcast.github.io/">
              <papertitle>Broadcasting Support Relations Recursively from Local Dynamics for Object Retrieval in Clutters
              </papertitle>
              </a>
              <br>
              <a href="https://lyttttt3333.github.io/YitongLi.github.io/">Yitong Li</a>*,
              <strong>Ruihai Wu</strong>*,
              Haoran Lu,
              <a href="https://tritiumr.github.io/">Chuanruo Ning</a>,
              <a href="https://sxy7147.github.io/">Yan Shen</a>,
              <a href="https://www.robots.ox.ac.uk/~guanqi/">Guanqi Zhan</a>,
              <a href="http://zsdonghao.github.io/">Hao Dong</a>
              <br>
              <em>RSS 2024</em>
              <br>
               <b style="color: red;">Best Poster Award</b> at <a href="https://mp.weixin.qq.com/s/0NRnQqnT4Y9u8-AV1EaZQw">PKU AI Tech Day 2024</a>
              <br>
              <a href="https://lyttttt3333.github.io/broadcast.github.io/">project page</a>
              /
              <a href="https://arxiv.org/abs/2406.02283">paper</a>
<!--              paper (coming soon)-->
              /
              <a href="https://github.com/lyttttt3333/Broadcast_Support_Relation">code</a>
<!--              code (coming soon)-->
              /
<!--              video (coming soon)-->
              <a href="https://youtu.be/vUCDVfrvOrg">video</a>
              <p></p>
              <p>In this paper, we study retrieving objects in complicated clutters via a novel method of recursively broadcasting the accurate local dynamics to build a support relation graph of the whole scene, which largely reduces the complexity of the support relation inference and improves the accuracy. </p>
            </td>
          </tr>




          <tr onmouseout="garmentlab_stop()" onmouseover="garmentlab_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='garmentlab_image'>
                  <img src='images/unigarment.png' width="187"></div>
                <img src='images/unigarment.png' width="187">
              </div>
              <script type="text/javascript">
                function garmentlab_start() {
                  document.getElementById('garmentlab_image').style.opacity = "1";
                }

                function garmentlab_stop() {
                  document.getElementById('garmentlab_image').style.opacity = "0";
                }
                garmentlab_stop()
              </script>
            </td>

            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://garmentlab.github.io/">
              <papertitle>GarmentLab: A Unified Simulation and Benchmark for Garment Manipulation
              </papertitle>
              </a>
              <br>
              <a href="https://luhr2003.github.io/">Haoran Lu</a>*,
              <strong>Ruihai Wu</strong>*,
              <a href="https://lyttttt3333.github.io/YitongLi.github.io/">Yitong Li</a>*,
              Sijie Li,
              <a href="https://alwaysleepy.github.io/">Ziyu Zhu</a>*,
              <a href="https://tritiumr.github.io/">Chuanruo Ning</a>,
              <a href="https://sxy7147.github.io/">Yan Shen</a>,
              Longzan Luo,
              <a href="https://cypypccpy.github.io/">Yuanpei Chen</a>,
              <a href="http://zsdonghao.github.io/">Hao Dong</a>
              <br>
              <em>NeurIPS 2024</em>
              <br>
               <b style="color: red;">Spotlight Presentation</b> at <a href="https://deformable-workshop.github.io/icra2024/">ICRA 2024 Workshop on Deformable Object Manipulation</a>
              <br>
              <a href="https://garmentlab.github.io/">project page</a>
              /
              <a href="https://arxiv.org/abs/2411.01200">paper</a>
              /
              <a href="https://github.com/GarmentLab/GarmentLab">code</a>
              /
              <a href="https://garmentlab.github.io/static/videos/ID_7401.mp4">video</a>
              <p></p>
              <p>We present GarmentLab, a benchmark designed for garment manipulation within realistic 3D indoor scenes. Our benchmark encompasses a diverse range of garment types, robotic systems and manipulators including dexterous hands. The multitude of tasks included in the benchmark enables further exploration of the interactions between garments, deformable objects, rigid bodies, fluids, and avatars.  </p>
            </td>
          </tr>

<!--          <tr onmouseout="roboexp_stop()" onmouseover="roboexp_start()">-->
<!--            <td style="padding:20px;width:25%;vertical-align:middle">-->
<!--              <div class="one">-->
<!--                <div class="two" id='roboexp_image'>-->
<!--                  <img src='images/roboexp.gif' width="187"></div>-->
<!--                <img src='images/roboexp.gif' width="187">-->
<!--              </div>-->
<!--              <script type="text/javascript">-->
<!--                function roboexp_start() {-->
<!--                  document.getElementById('roboexp_image').style.opacity = "1";-->
<!--                }-->

<!--                function roboexp_stop() {-->
<!--                  document.getElementById('roboexp_image').style.opacity = "0";-->
<!--                }-->
<!--                roboexp_stop()-->
<!--              </script>-->
<!--            </td>-->

<!--            <td style="padding:20px;width:75%;vertical-align:middle">-->
<!--              <a href="https://jianghanxiao.github.io/roboexp-web/">-->
<!--              <papertitle>RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for Robotic Manipulation-->
<!--              </papertitle>-->
<!--              </a>-->
<!--              <br>-->
<!--              <a href="https://jianghanxiao.github.io/">Hanxiao Jiang</a>,-->
<!--              <a href="https://binghao-huang.github.io/">Binghao Huang</a>,-->
<!--              <strong>Ruihai Wu</strong>,-->
<!--              <a href="https://sg.linkedin.com/in/zhuoran-li-david">Zhuoran Li</a>,-->
<!--              <a href="https://www.gargshubham.com/">Shubham Garg</a>,-->
<!--              <a href="https://www.amazon.science/author/hooshang-nayyeri">Hooshang Nayyeri</a>,-->
<!--              <a href="https://shenlong.web.illinois.edu/">Shenlong Wang</a>,-->
<!--              <a href="https://yunzhuli.github.io/">Yunzhu Li</a>-->
<!--              <br>-->
<!--              <em>CoRL 2024</em>-->
<!--              <br>-->
<!--              <b style="color: red;">Best Paper Nomination</b> at <a href="https://vlmnm-workshop.github.io/">ICRA 2024 Workshop on Vision-Language Models for Manipulation</a>-->
<!--              <br>-->
<!--              <a href="https://jianghanxiao.github.io/roboexp-web/">project page</a>-->
<!--              /-->
<!--              <a href="https://arxiv.org/abs/2402.15487">paper</a>-->
<!--              /-->
<!--              <a href="https://github.com/Jianghanxiao/RoboEXP">code</a>-->
<!--              /-->
<!--              <a href="https://youtu.be/xZ1gfLRXSOE">video</a>-->
<!--              <p></p>-->
<!--              <p>We formulate interactive exploration as an action-conditioned 3D scene graph (ACSG) construction and traversal problem. Our ACSG is an actionable, spatial-topological representation that models objects and their interactive and spatial relations in a scene, capturing both the high-level graph and corresponding low-level memory. </p>-->
<!--            </td>-->
<!--          </tr>-->



          <tr onmouseout="preafford_stop()" onmouseover="preafford_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='preafford_image'>
                  <img src='images/preafford.png' width="187"></div>
                <img src='images/preafford.png' width="187">
              </div>
              <script type="text/javascript">
                function preafford_start() {
                  document.getElementById('preafford_image').style.opacity = "1";
                }

                function preafford_stop() {
                  document.getElementById('preafford_image').style.opacity = "0";
                }
                preafford_stop()
              </script>
            </td>

            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://air-discover.github.io/PreAfford/">
              <papertitle>PreAfford: Universal Affordance-Based Pre-Grasping for Diverse Objects and Environments
              </papertitle>
              </a>
              <br>
              <a href="https://robot-k.github.io/">Kairui Ding</a>,
              Boyuan Chen,
              <strong>Ruihai Wu</strong>,
              <a href="https://yuyangli.com/">Yuyang Li</a>,
              Zongzheng Zhang,
              <a href="https://c7w.tech/about/">Huan-ang Gao</a>,
              Siqi Li,
              <a href="www.yixin.io/">Yixin Zhu</a>,
              <a href="https://air.tsinghua.edu.cn/en/info/1046/1196.htm">Guyue Zhou</a>,
              <a href="http://zsdonghao.github.io/">Hao Dong</a>,
              <a href="http://47.100.254.234:1008/">Hao Zhao</a>
              <br>
              <em>IROS 2024</em>
              <br>
               <b style="color: red;">Best Poster Finalist</b> at <a href="https://edmws.github.io/">IROS 2024 Workshop on Embodied Navigation to Movable Objects</a>
              <br>
              <a href="https://air-discover.github.io/PreAfford/">project page</a>
              /
              <a href="https://arxiv.org/abs/2404.03634">paper</a>
<!--              paper (coming soon)-->
              /
              <a href="https://github.com/Robot-K/PreAfford">code</a>
<!--              code (coming soon)-->
              /
<!--              video (coming soon)-->
              <a href="https://air-discover.github.io/PreAfford/static/videos/intro_website_ffmpeg.mp4">video</a>
              <p></p>
              <p>PreAfford is a novel pre-grasping planning framework that improves adaptability across diverse environments and object by utilizing point-level affordance representation and relay training. Validated on the ShapeNet-v2 dataset and real-world experiments, PreAfford offers a robust solution for manipulating ungraspable objects with two-finger grippers. </p>
            </td>
          </tr>



          <tr onmouseout="naturalvlm_stop()" onmouseover="naturalvlm_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='naturalvlm_image'>
                  <img src='images/naturalVLM.gif' width="187"></div>
                <img src='images/naturalVLM.gif' width="187">
              </div>
              <script type="text/javascript">
                function naturalvlm_start() {
                  document.getElementById('naturalvlm_image').style.opacity = "1";
                }

                function naturalvlm_stop() {
                  document.getElementById('naturalvlm_image').style.opacity = "0";
                }
                naturalvlm_stop()
              </script>
            </td>

            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://sites.google.com/view/naturalvlm">
              <papertitle>NaturalVLM: Leveraging Fine-grained Natural Language for Affordance-Guided Visual Manipulation
              </papertitle>
              </a>
              <br>
              Ran Xu*,
              <a href="https://sxy7147.github.io/">Yan Shen</a>*,
              <a href="https://scholar.google.com/citations?hl=en&user=vkQ5_LIAAAAJ">Xiaoqi Li</a>,
              <strong>Ruihai Wu</strong>,
              <a href="http://zsdonghao.github.io/">Hao Dong</a>
              <br>
              <em>RA-L 2024</em>
              <br>
              <a href="https://sites.google.com/view/naturalvlm">project page</a>
              /
              <a href="https://arxiv.org/abs/2403.08355">paper</a>
              /
              <a href="https://drive.google.com/file/d/1BOryJUt2xqvLryvByVBtZ1Ym_R3HRkRK/view">video</a>
              <p></p>
              <p>We introduce a comprehensive benchmark, NaturalVLM, comprising 15 distinct manipulation tasks, containing over 4500 episodes meticulously annotated with fine-grained language instructions. Besides, we propose a novel learning framework that completes the manipulation task step-by-step according to the fine-grained instructions. </p>
            </td>
          </tr>

          <tr onmouseout="env_stop()" onmouseover="env_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='env_image'>
                  <img src='images/env.jpg' width="187"></div>
                <img src='images/env.jpg' width="187">
              </div>
              <script type="text/javascript">
                function env_start() {
                  document.getElementById('env_image').style.opacity = "1";
                }

                function env_stop() {
                  document.getElementById('env_image').style.opacity = "0";
                }
                env_stop()
              </script>
            </td>

            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://neurips.cc/virtual/2023/poster/71652">
              <papertitle>Learning Environment-Aware Affordance for 3D Articulated Object Manipulation under Occlusions
              </papertitle>
              </a>
              <br>
              <strong>Ruihai Wu</strong>*,
              <a href="https://chengkaiacademycity.github.io/">Kai Cheng</a>*,
              <a href="https://sxy7147.github.io/">Yan Shen</a>,
              <a href="https://tritiumr.github.io/">Chuanruo Ning</a>,
              <a href="https://www.robots.ox.ac.uk/~guanqi/">Guanqi Zhan</a>,
              <a href="http://zsdonghao.github.io/">Hao Dong</a>
              <br>
              <em>NeurIPS 2023</em>
              <br>
              <a href="https://chengkaiacademycity.github.io/EnvAwareAfford/">project page</a>
              /
              <a href="https://neurips.cc/virtual/2023/poster/71652">paper</a>
              /
              <a href="https://github.com/chengkaiAcademyCity/EnvAwareAfford">code</a>
              /
              <a href="https://youtu.be/r88hWcRmQYM">video</a>
              <p></p>
              <p>We explore the task of manipulating articulated objects within environment constraints and formulate the task of environment-aware affordance learning for manipulating 3D articulated objects, incorporating object-centric per-point priors and environment constraints. </p>
            </td>
          </tr>


          <tr onmouseout="where2explore_stop()" onmouseover="where2explore_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='where2explore_image'>
                  <img src='images/where2explore.png' width="187"></div>
                <img src='images/where2explore.png' width="187">
              </div>
              <script type="text/javascript">
                function where2explore_start() {
                  document.getElementById('where2explore_image').style.opacity = "1";
                }

                function where2explore_stop() {
                  document.getElementById('where2explore_image').style.opacity = "0";
                }
                where2explore_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://neurips.cc/virtual/2023/poster/71709">
              <papertitle>Where2Explore: Few-shot Affordance Learning for Unseen Novel Categories of Articulated Objects
              </papertitle>
              </a>
              <br>
              <a href="https://tritiumr.github.io/">Chuanruo Ning</a>,
              <strong>Ruihai Wu</strong>,
              <a href="https://luhr2003.github.io/">Haoran Lu</a>,
              <a href="https://kaichun-mo.github.io/">Kaichun Mo</a>,
              <a href="http://zsdonghao.github.io/">Hao Dong</a>
              <br>
              <em>NeurIPS 2023</em>
              <br>
              <a href="https://tritiumr.github.io/Where2Explore/">project page</a>
              /
              <a href="https://neurips.cc/virtual/2023/poster/71709">paper</a>
              /
              <a href="https://github.com/TritiumR/Where2Explore">code</a>
              /
              <a href="https://youtu.be/NKfUewWAors">video</a>
              <p></p>
              <p>We introduce an affordance learning framework that effectively explores novel categories with minimal interactions on a limited number of instances. Our framework explicitly estimates the geometric similarity across different categories, identifying local areas that differ from shapes in the training categories for efficient exploration while concurrently transferring affordance knowledge to similar parts of the objects. </p>
            </td>
          </tr>



          <tr onmouseout="assembly_stop()" onmouseover="assembly_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='assembly_image'>
                  <img src='images/assembly.gif' width="187"></div>
                <img src='images/assembly.gif' width="187">
              </div>
              <script type="text/javascript">
                function assembly_start() {
                  document.getElementById('assembly_image').style.opacity = "1";
                }

                function assembly_stop() {
                  document.getElementById('assembly_image').style.opacity = "0";
                }
                assembly_stop()
              </script>
            </td>

            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://crtie.github.io/SE-3-part-assembly/">
              <papertitle>Leveraging SE(3) Equivariance for Learning 3D Geometric Shape Assembly
              </papertitle>
              </a>
              <br>
              <strong>Ruihai Wu</strong>*,
              <a href="https://crtie.github.io/">Chenrui Tie</a>*,
              <a href="https://scholar.google.com/citations?user=x8zGkt0AAAAJ&hl=en">Yushi Du</a>,
              <a href="https://sxy7147.github.io/">Yan Shen</a>,
              <a href="http://zsdonghao.github.io/">Hao Dong</a>
              <br>
              <em>ICCV 2023</em>
              <br>
              <a href="https://crtie.github.io/SE-3-part-assembly/">project page</a>
              /
              <a href="https://arxiv.org/abs/2309.06810">paper</a>
<!--              paper (coming soon)-->
              /
              <a href="https://github.com/crtie/Leveraging-SE-3-Equivariance-for-Learning-3D-Geometric-Shape-Assembly">code</a>
<!--              code (coming soon)-->
              /
              <a href="https://youtu.be/pEtIAal-xgQ">video</a>
              <p></p>
              <p>We study geometric shape assembly by leveraging SE(3) Equivariance, which disentangles poses and shapes of fractured parts. </p>
            </td>
          </tr>


          <tr onmouseout="deformable_stop()" onmouseover="deformable_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='deformable_image'>
                  <img src='images/deformable.png' width="187"></div>
                <img src='images/deformable.png' width="187">
              </div>
              <script type="text/javascript">
                function deformable_start() {
                  document.getElementById('deformable_image').style.opacity = "1";
                }

                function deformable_stop() {
                  document.getElementById('deformable_image').style.opacity = "0";
                }
                deformable_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://hyperplane-lab.github.io/DeformableAffordance/">
              <papertitle>Learning Foresightful Dense Visual Affordance for Deformable Object Manipulation
              </papertitle>
              </a>
              <br>
              <strong>Ruihai Wu</strong>*,
              <a href="https://tritiumr.github.io/">Chuanruo Ning</a>*,
              <a href="http://zsdonghao.github.io/">Hao Dong</a>
              <br>
              <em>ICCV 2023</em>
              <br>
              <a href="https://hyperplane-lab.github.io/DeformableAffordance/">project page</a>
              /
              <a href="https://arxiv.org/abs/2303.11057">paper</a>
              /
              <a href="https://github.com/TritiumR/DeformableAffordance">code</a>
              /
              <a href="https://youtu.be/DiZ9aXjK_PU">video</a>
              /
              <a href="https://youtu.be/aYneBzwhOGs">video (real world)</a>
              <p></p>
              <p>We study deformable object manipulation using dense visual affordance, with generalization towards diverse states, and propose a novel kind of foresightful dense affordance, which avoids local optima by estimating states’ values for longterm manipulation. </p>
            </td>
          </tr>

             <tr onmouseout="dualafford_stop()" onmouseover="dualafford_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='dualafford_image'>
                  <img src='images/2022-dualafford.gif' width="187"></div>
                <img src='images/2022-dualafford.gif' width="187">
              </div>
              <script type="text/javascript">
                function dualafford_start() {
                  document.getElementById('dualafford_image').style.opacity = "1";
                }

                function dualafford_stop() {
                  document.getElementById('dualafford_image').style.opacity = "0";
                }
                dualafford_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://hyperplane-lab.github.io/DualAfford/">
              <papertitle>DualAfford: Learning Collaborative Visual Affordance for Dual-gripper Object Manipulation
              </papertitle>
              </a>
              <br>
              <a href="https://sxy7147.github.io/">Yan Zhao</a>*,
              <strong>Ruihai Wu</strong>*,
              <a href="https://acmlczh.github.io/">Zhehuan Chen</a>,
              <a href="https://www.linkedin.com/in/yourong-zhang-2b1aab23a/">Yourong Zhang</a>,
              <a href="https://fqnchina.github.io/">Qingnan Fan</a>,
              <a href="https://kaichun-mo.github.io/">Kaichun Mo</a>,
              <a href="http://zsdonghao.github.io/">Hao Dong</a>
              <br>
              <em>ICLR 2023</em>
              <br>
              <a href="https://hyperplane-lab.github.io/DualAfford/">project page</a>
              /
              <a href="https://arxiv.org/pdf/2207.01971.pdf">paper</a>
              /
              <a href="https://github.com/sxy7147/DualAfford">code</a>
              /
              <a href="https://youtu.be/J0UUE2FvOCE">video</a>
              <p></p>
              <p>We study collaborative affordance for dual-gripper manipulation. The core is to reduce the quadratic problem for two grippers into two disentangled yet interconnected subtasks. </p>
            </td>
          </tr>



          <tr onmouseout="assembly_stop()" onmouseover="assembly_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='dpart_image'>
                  <img src='images/dpart2.png' width="187"></div>
                <img src='images/dpart2.png' width="187">
              </div>
              <script type="text/javascript">
                function dpart_start() {
                  document.getElementById('dpart_image').style.opacity = "1";
                }

                function dpart_stop() {
                  document.getElementById('dpart_image').style.opacity = "0";
                }
                dpart_stop()
              </script>
            </td>

            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://Yushi-Du.github.io/PartMotion/">
              <papertitle>Learning Part Motion of Articulated Objects Using Spatially Continuous Neural Implicit Representations
              </papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=x8zGkt0AAAAJ&hl=en">Yushi Du</a>*,
              <strong>Ruihai Wu</strong>*,
              <a href="https://sxy7147.github.io/">Yan Shen</a>,
              <a href="http://zsdonghao.github.io/">Hao Dong</a>
              <br>
              <em>BMVC 2023</em>
              <br>
              <a href="https://yushi-du.github.io/PartMotion/">project page</a>
              /
              <a href="https://arxiv.org/abs/2311.12407">paper</a>
<!--              paper (coming soon)-->
              /
              <a href="https://github.com/Yushi-Du/PartMotion">code</a>
<!--              code (coming soon)-->
              /
              <a href="https://youtu.be/aY5-maonUBU">video</a>
              <p></p>
              <p>We introduce a novel framework that explicitly disentangles the part motion of articulated objects by predicting the movements of articulated parts by utilizing spatially continuous neural implicit representations. </p>
            </td>
          </tr>

             <tr onmouseout="adaafford_stop()" onmouseover="adaafford_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='adaafford_image'>
                  <img src='images/eccv_2022_AdaAfford.gif' width="187"></div>
                <img src='images/eccv_2022_AdaAfford.gif' width="187">
              </div>
              <script type="text/javascript">
                function adaafford_start() {
                  document.getElementById('adaafford_image').style.opacity = "1";
                }

                function adaafford_stop() {
                  document.getElementById('adaafford_image').style.opacity = "0";
                }
                adaafford_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://hyperplane-lab.github.io/AdaAfford/">
              <papertitle>AdaAfford: Learning to Adapt Manipulation Affordance for 3D Articulated Objects via Few-shot Interactions
              </papertitle>
              </a>
              <br>
              <a href="https://wangyian-me.github.io/">Yian Wang</a>*,
              <strong>Ruihai Wu</strong>*,
              <a href="https://kaichun-mo.github.io/">Kaichun Mo</a>*,
              Jiaqi Ke,
              <a href="https://fqnchina.github.io/">Qingnan Fan</a>,
              <a href="https://geometry.stanford.edu/member/guibas/">Leonidas J. Guibas</a>,
              <a href="http://zsdonghao.github.io/">Hao Dong</a>
              <br>
              <em>ECCV 2022</em>
              <br>
              <a href="https://hyperplane-lab.github.io/AdaAfford/">project page</a>
              /
              <a href="https://arxiv.org/pdf/2112.00246.pdf">paper</a>
              /
              <a href="https://github.com/wangyian-me/AdaAffordCode/">code</a>
              /
              <a href="https://youtu.be/Qaq0YkpNxe4">video</a>
              <p></p>
              <p>We study performing very few test-time interactions for quickly adapting the affordance priors to more accurate instance-specific posteriors. </p>
            </td>
          </tr>


             <tr onmouseout="vatmart_stop()" onmouseover="vatmart_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='vatmart_image'>
                  <img src='images/vat_mart.gif' width="187"></div>
                <img src='images/vat_mart.gif' width="187">
              </div>
              <script type="text/javascript">
                function vatmart_start() {
                  document.getElementById('vatmart_image').style.opacity = "1";
                }

                function vatmart_stop() {
                  document.getElementById('vatmart_image').style.opacity = "0";
                }
                vatmart_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://hyperplane-lab.github.io/vat-mart/">
              <papertitle>VAT-Mart: Learning Visual Action Trajectory Proposals for Manipulating 3D ARTiculated Objects
              </papertitle>
              </a>
              <br>
              <strong>Ruihai Wu</strong>*,
              <a href="https://sxy7147.github.io/">Yan Zhao</a>*,
              <a href="https://kaichun-mo.github.io/">Kaichun Mo</a>*,
              <a href="https://guozz.cn/">Zizheng Guo</a>,
              <a href="https://wangyian-me.github.io/">Yian Wang</a>,
              <a href="https://tianhaowuhz.github.io/">Tianhao Wu</a>,
              <a href="https://fqnchina.github.io/">Qingnan Fan</a>,
              <a href="https://xuelin-chen.github.io/">Xuelin Chen</a>,
              <a href="https://geometry.stanford.edu/member/guibas/">Leonidas J. Guibas</a>,
              <a href="http://zsdonghao.github.io/">Hao Dong</a>
              <br>
              <em>ICLR 2022</em>
              <br>
              <a href="https://hyperplane-lab.github.io/vat-mart/">project page</a>
              /
              <a href="https://arxiv.org/pdf/2106.14440.pdf">paper</a>
              /
              <a href="https://github.com/warshallrho/VAT-Mart">code</a>
              /
              <a href="https://www.youtube.com/watch?v=HjhsLKf1eQY">video</a>
              <p></p>
              <p>We study dense geometry-aware, interaction-aware, and task-aware visual action affordance and trajectory proposals for manipulating articulated objects. </p>
            </td>
          </tr>

                       <tr onmouseout="dmotion_stop()" onmouseover="dmotion_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='dmotion_image'>
                  <img src='images/2021dmotion.gif' width="187"></div>
                <img src='images/2021dmotion.gif' width="187">
              </div>
              <script type="text/javascript">
                function dmotion_start() {
                  document.getElementById('dmotion_image').style.opacity = "1";
                }
                function dmotion_stop() {
                  document.getElementById('dmotion_image').style.opacity = "0";
                }
                dmotion_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://hyperplane-lab.github.io/dmotion/">
              <papertitle>DMotion: Robotic Visuomotor Control with Unsupervised Forward Model Learned from Videos
              </papertitle>
              </a>
              <br>
              <a href="https://yhqpkueecs.github.io/">Haoqi Yuan</a>*,
              <strong>Ruihai Wu</strong>*,
              <a href="https://andrewzh112.github.io/">Andrew Zhao</a>*,
              Haipeng Zhang,
              <a href="https://quantumiracle.github.io/webpage/">Zihan Ding</a>,
              <a href="http://zsdonghao.github.io/">Hao Dong</a>
              <br>
              <em>IROS 2021</em>
              <br>
              <a href="https://hyperplane-lab.github.io/dmotion/">project page</a>
              /
              <a href="https://arxiv.org/pdf/2103.04301.pdf">paper</a>
              /
              <a href="https://github.com/hyperplane-lab/dmotion-code">code</a>
              <p></p>
              <p>We train a forward model from video data only, via disentangling the motion of controllable agent to model the transition dynamics. </p>
            </td>
          </tr>

             <tr onmouseout="aclgan_stop()" onmouseover="aclgan_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='aclgan_image'>
                  <img src='images/2020aclgan.gif' width="187"></div>
                <img src='images/2020aclgan.gif' width="187">
              </div>
              <script type="text/javascript">
                function aclgan_start() {
                  document.getElementById('aclgan_image').style.opacity = "1";
                }

                function aclgan_stop() {
                  document.getElementById('aclgan_image').style.opacity = "0";
                }
                aclgan_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://hyperplane-lab.github.io/acl-gan-page/">
              <papertitle>Unpaired Image-to-Image Translation using Adversarial Consistency Loss
              </papertitle>
              </a>
              <br>
              <a href="https://rivendile.github.io/">Yihao Zhao</a>,
              <strong>Ruihai Wu</strong>,
              <a href="http://zsdonghao.github.io/">Hao Dong</a>
              <br>
              <em>ECCV 2020</em>
              <br>
              <a href="https://hyperplane-lab.github.io/acl-gan-page/">project page</a>
              /
              <a href="https://arxiv.org/abs/2003.04858">paper</a>
              /
              <a href="https://github.com/hyperplane-lab/ACL-GAN">code</a>
              /
              <a href="https://youtu.be/mB2XfpkHuMQ">video</a>
              <p></p>
              <p> We propose adversarial consistency loss for image-to-image translation that does not require the translated image to be translated back to the source image. </p>
            </td>
          </tr>


             <tr onmouseout="vrd_stop()" onmouseover="vrd_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='vrd_image'>
                  <img src='images/vrd.gif' width="187"></div>
                <img src='images/vrd.gif' width="187">
              </div>
              <script type="text/javascript">
                function vrd_start() {
                  document.getElementById('vrd_image').style.opacity = "1";
                }

                function vrd_stop() {
                  document.getElementById('vrd_image').style.opacity = "0";
                }
                vrd_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6913">
              <papertitle>Localize, Assemble, and Predicate: Contextual Object Proposal Embedding for Visual Relation Detection
              </papertitle>
              </a>
              <br>
              <strong>Ruihai Wu</strong>,
              <a href="https://fiona730.github.io/">Kehan Xu</a>,
              Chenchen Liu,
              Nan Zhuang,
              <a href="http://www.muyadong.com/">Yadong Mu</a>
              <br>
              <em>AAAI 2020</em>
              <b style="color: red;">(Oral presentation) </b>
              <br>
              <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6913">paper</a>
              <p></p>
              <p> We propose localize-assemble-predicate network (LAP-Net), decomposing visual relation detection (VRD) into three sub-tasks to tackle the long-tailed data distribution problem. </p>
            </td>
          </tr>

             <tr onmouseout="tdmpnet_stop()" onmouseover="tdmpnet_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='tdmpnet_image'>
                  <img src='images/tdmpnet.png' width="187"></div>
                <img src='images/tdmpnet.png' width="187">
              </div>
              <script type="text/javascript">
                function tdmpnet_start() {
                  document.getElementById('tdmpnet_image').style.opacity = "1";
                }

                function tdmpnet_stop() {
                  document.getElementById('tdmpnet_image').style.opacity = "0";
                }
                tdmpnet_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openreview.net/forum?id=v_KSmk9B5kt">
              <papertitle>TDMPNet: Prototype Network with Recurrent Top-Down Modulation for Robust Object Classification under Partial Occlusion
              </papertitle>
              </a>
              <br>
              <a href="https://pkuxmq.github.io/">Mingqing Xiao</a>,
              <a href="https://adamkortylewski.com/">Adam Kortylewski</a>,
              <strong>Ruihai Wu</strong>,
              <a href="https://www.cs.jhu.edu/~syqiao/">Siyuan Qiao</a>,
              <a href="https://shenwei1231.github.io/">Wei Shen</a>,
              <a href="https://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>
              <br>
              <em>ECCV 2020 <a href="https://openreview.net/group?id=thecvf.com/ECCV/2020/Workshop/VIPriors">Visual Inductive Priors for Data-Efficient Deep Learning Workshop </a> </em>
              <br>
              <a href="https://openreview.net/forum?id=v_KSmk9B5kt">paper</a>
              <p></p>
              <p> We introduce prototype learning, partial matching and convolution layers with top-down modulation into feature extraction to purposefully reduce the contamination by occlusion. </p>
            </td>
          </tr>

        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Other Projects</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <tr onmouseout="tensorlayer_stop()" onmouseover="tensorlayer_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='tensorlayer_image'>
                  <img src='images/tl_transparent_logo.png' width="187"></div>
                <img src='images/tl_transparent_logo.png' width="187">
              </div>
              <script type="text/javascript">
                function tensorlayer_start() {
                  document.getElementById('tensorlayer_image').style.opacity = "1";
                }

                function tensorlayer_stop() {
                  document.getElementById('tensorlayer_image').style.opacity = "0";
                }
                tensorlayer_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://github.com/tensorlayer/TensorLayer">
              <papertitle>TensorLayer
              </papertitle>
              </a>
              <br>
              <br>
              A deep learning and reinforcement learning library designed for researchers and engineers.
              <br>
              <a href="https://twitter.com/ImperialDSI/status/923928895325442049">ACM MM Best Open Source Software Award, <em>2017</em>. </a>
              <br>
              I am one of the main contributors of its 2.0 version.
              <br>
              <br>
              <a href="https://github.com/tensorlayer/TensorLayer">GitHub</a>
              /
              <a href="https://github.com/tensorlayer/TensorLayer/stargazers">star (7000+)</a>
              /
              <a href="https://github.com/tensorlayer/TensorLayer/forks">fork (1600+)</a>
              /
              <a href="https://github.com/tensorlayer/TensorLayer/graphs/contributors">contributors</a>
              <p></p>
            </td>
          </tr>

          <tr onmouseout="spreadsheet_stop()" onmouseover="spreadsheet_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='spreadsheet_image'>
                  <img src='images/spreadsheet.gif' width="187"></div>
                <img src='images/spreadsheet.gif' width="187">
              </div>
              <script type="text/javascript">
                function spreadsheet_start() {
                  document.getElementById('spreadsheet_image').style.opacity = "1";
                }

                function spreadsheet_stop() {
                  document.getElementById('spreadsheet_image').style.opacity = "0";
                }
                spreadsheet_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.microsoft.com/en-us/research/project/spreadsheet-intelligence/">
              <papertitle>Spreadsheet Intelligence (Microsoft Research Asia)
              </papertitle>
              </a>
              <br>
              <br>
              Umbrella research project behind <a href="https://support.microsoft.com/en-us/office/analyze-data-in-excel-3223aab8-f543-4fda-85ed-76bb0295ffc4?ui=en-us&rs=en-us&ad=us">Ideas in Excel</a> of Microsoft Office 365 product.
              <br>
              Intelligent feature announced at Microsoft Ignite Conference and released on March, 2019.
              <br>
              Star of tomorrow excellent intern, <em>2019</em>.
              <br>
              <br>
              <a href="https://www.microsoft.com/en-us/research/project/spreadsheet-intelligence/">project page</a>
              <p></p>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Services</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td width="75%" valign="center">
              Reviewer: ICCV, CVPR, ICLR, ICRA, RA-L, T-RO, T-Mech, TVCG
              <br>
              Volunteer: <a href="images/wine2020.pdf">WINE <em>2020</em></a>
            </td>
          </tr>
        </tbody></table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Teaching</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>

          <tr>
            <td width="75%" valign="center">
                Teaching Assistant:
            <br>
              &nbsp &nbsp &nbsp <a href="https://deep-generative-models.github.io/">Deep Generative Models</a>, <em> 2020, 2022 </em>
            <br>
                Guest Lecturer:
            <br>
              &nbsp &nbsp &nbsp Frontier Computing Research Practice (Course of Open Source Development), <em> 2024 </em>
            <br>
              &nbsp &nbsp &nbsp Introduction to Computing (Course of Dynamic Programming), <em> 2024 </em>
            </td>
          </tr>

        </tbody></table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Invited Talks</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td width="75%" valign="center">
              Learning 3D Visual Representations for Robotic Manipulation,
              <br>
              &nbsp &nbsp &nbsp <em>National University of Singapore, Feb. 2025</em>
              <br>
              &nbsp &nbsp &nbsp <em>Beijing Normal University, Shanghai Tech University, TeleAI, Jan. 2025</em>
              <br>
              &nbsp &nbsp &nbsp <em>Tsinghua University, Johns Hopkins University, Carnegie Mellon University, Dec. 2024</em>
              <br>
              &nbsp &nbsp &nbsp <em>University of Hong Kong, Nov. 2024</em>
              <br>
              Unified Simulation, Benchmark and Manipulation for Garments, &nbsp&nbsp&nbsp  <em><a href="https://anysyn3d.github.io/about.html">AnySyn3D</a>, 2024</em>
              <br>
                &nbsp &nbsp &nbsp --- If you are interested in 3D Vision research, welcome to follow <a href="https://anysyn3d.github.io/about.html">AnySyn3D</a> that conducts various topics.
              <br>
              Visual Representations for Embodied Agent, &nbsp&nbsp&nbsp  <em>Chinese University of Hong Kong, Shenzhen, 2024</em>
              <br>
              <a href="http://www.csig3dv.net/2024/studentFotum.html">Visual Representations for Embodied Agent</a>, &nbsp&nbsp&nbsp  <em>China3DV, 2024</em>
              <br>
            </td>
          </tr>

        </tbody></table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Honors and Awards</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td width="75%" valign="center">
              <br>
              Second Prize (top3), Outstanding Doctor Award of <a href="http://ceai.caai.cn/">China Embodied AI Conference (CEAI)</a>, &nbsp&nbsp&nbsp  <em>China, 2025</em>
              <br>
              Nomination Award (top3), Outstanding Young Scholar Paper Award of <a href="http://ceai.caai.cn/">China Embodied AI Conference (CEAI)</a>, &nbsp&nbsp&nbsp  <em>China, 2025</em>
              <br>
              Nomination Award, Annual Rising Star Award of <a href="http://china3dv.csig.org.cn/">China3DV</a>, &nbsp&nbsp&nbsp  <em>China, 2025</em>
              <br>
              <a href="https://ur.bytedance.com/scholarship">ByteDance Scholarship</a>, &nbsp&nbsp&nbsp  <em>China and Singapore, 2024</em>
              <br>
              Nomination Award (top8), <a href="https://www.roboarts.cn/index">ARTS Scholarship</a>， &nbsp&nbsp&nbsp  <em>China, 2024</em>
              <br>
              National Scholarship and Merit Student (top1 in 50, CFCS), &nbsp&nbsp&nbsp  <em>Peking University, Chinese Ministry of Education, 2024</em>
              <br>
              Outstanding Student Workshop Speaker (top8), <a href="https://conf.csig.org.cn/10389.html">China3DV</a>, &nbsp&nbsp&nbsp  <em>China, 2024</em>
              <br>
              Nomination, <a href="https://machinelearning.apple.com/updates/apple-scholars-aiml-2024">Apple AI/ML Scholar Fellowship</a>, &nbsp&nbsp&nbsp  <em>WorldWide, 2024</em>
              <br>
<!--              Finalist, <a href="https://ur.bytedance.com/scholarship">ByteDance Scholarship</a>, &nbsp&nbsp&nbsp  <em>China, 2023</em>-->
<!--              <br>-->
<!--              Jiukun Scholarship (10 in School of CS), &nbsp&nbsp&nbsp  <em>Peking University, 2023</em>-->
<!--              <br>-->
              Research Excellence Award, &nbsp&nbsp&nbsp  <em>Peking University, 2019, 2022, 2023</em>
              <br>
              Excellent Graduate, &nbsp&nbsp&nbsp  <em>Peking University, 2020 (undergrad), 2025 (Ph.D.)</em>
              <br>
              Peking University Scholarship Third Prize, &nbsp&nbsp&nbsp  <em>Peking University, 2019</em>
<!--              <br>-->
<!--              Research Excellence Award, &nbsp&nbsp&nbsp  <em>Peking University, 2019</em>-->
              <br>
              Star of Tomorrow Excellent Intern, &nbsp&nbsp&nbsp  <em>Microsoft Research Asia, 2019</em>
              <br>
              May Fourth Scholarship and Academic Excellence Award, &nbsp&nbsp&nbsp  <em>Peking University, 2018</em>
              <br>
              Bronze medal in National Olympiad in Informatics (NOI), &nbsp&nbsp&nbsp  <em>China Computer Federation, 2015</em>
              <br>
              First prize in National Olympiad in Informatics in Provinces (NOIP), &nbsp&nbsp&nbsp  <em>China Computer Federation, 2013, 2014, 2015</em>
              <br>
            </td>
          </tr>

        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
                  <div style="float:right;">
                    Website template comes from <a href="https://jonbarron.info/">Jon Barron</a><br>
                      Last update: March, 2025
                  </div>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
